{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e91ae0fd",
   "metadata": {},
   "source": [
    "# Image registration (spatial alignment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e021628b",
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "import requests\n",
    "from tempfile import mkstemp\n",
    "from pathlib import Path\n",
    "\n",
    "from eddymotion.dmri import DWI\n",
    "from eddymotion.viz import plot_dwi\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "url = \"https://files.osf.io/v1/resources/8k95s/providers/osfstorage/6070b4c2f6585f03fb6123a2\"\n",
    "datapath = Path(mkstemp(suffix=\".h5\")[1])\n",
    "if datapath.stat().st_size == 0:\n",
    "    datapath.write_bytes(\n",
    "        requests.get(url, allow_redirects=True).content\n",
    "    )\n",
    "\n",
    "dmri_dataset = DWI.from_filename(datapath)\n",
    "datapath.unlink()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "852db625",
   "metadata": {},
   "source": [
    "At this point of the tutorial we have covered two of the three initial requirements:\n",
    "\n",
    "* we have a powerful data structure to access our dMRI dataset with agility, and\n",
    "* we have a reliable (thanks to DIPY!) model factory to generate *motion-less* references.\n",
    "\n",
    "Therefore, we are only one step away from our goal - aligning any given DW map with the *motion-less* reference.\n",
    "The estimation of the ***spatial transform*** that brings two maps into alignment is called ***image registration***.\n",
    "\n",
    "**Image registration** is therefore the process through which we bring the structural features of two images into alignment.\n",
    "This means that, brain sulci and gyri, the ventricles, subcortical structures, etc. are located exactly at the same place in the two images.\n",
    "That allows, for instance, for **image fusion**, and hence screening both images together (for example, applying some transparency to the one on top) should not give us the perception that they *are not aligned*.\n",
    "\n",
    "## ANTs - Advanced Normalization ToolS\n",
    "\n",
    "The [ANTs toolbox](http://stnava.github.io/ANTs/) is widely recognized as a powerful image registration (and *normalization*, which is registration to some standard space) framework.\n",
    "\n",
    "The output of an image registration process is the *estimated transform* that brings the information in the two images into alignment.\n",
    "In our case, the head-motion is a rigid-body displacement of the head.\n",
    "Therefore, a very simple (*linear*) model --an affine $4\\times 4$ matrix-- can be used to formalize the *estimated transforms*.\n",
    "\n",
    "Only very recently, [ANTs offers a Python interface](https://doi.org/10.1101/2020.10.19.20215392) to run their tools.\n",
    "For this reason, we will use the very much consolidated [*Nipype* wrapping of the ANTs' command-line interface](https://nipype.readthedocs.io/en/latest/api/generated/nipype.interfaces.ants.html#registration).\n",
    "The code is *almost* as simple as follows:\n",
    "\n",
    "```python\n",
    "from nipype.interfaces.ants import Registration\n",
    "\n",
    "registration_framework = Registration(\n",
    "    fixed_image=\"reference.nii.gz\",\n",
    "    moving_image=\"left-out-gradient.nii.gz\",\n",
    "    from_file=\"settings-file.json\"\n",
    ")\n",
    "```\n",
    "\n",
    "At the minimum, we need to establish our registration framework using the *fixed* (our synthetic, motion-less reference) and the *moving* (the left-out gradient) images.\n",
    "We can *easily* configure registration by creating a `settings-file.json` that may look like the following:\n",
    "\n",
    "```JSON\n",
    "{\n",
    "  \"collapse_output_transforms\": true,\n",
    "  \"convergence_threshold\": [ 1E-5, 1E-6 ],\n",
    "  \"convergence_window_size\": [ 5, 2 ],\n",
    "  \"dimension\": 3,\n",
    "  \"initialize_transforms_per_stage\": false,\n",
    "  \"interpolation\": \"BSpline\",\n",
    "  \"metric\": [ \"Mattes\", \"Mattes\" ],\n",
    "  \"metric_weight\": [ 1.0, 1.0 ],\n",
    "  \"number_of_iterations\": [\n",
    "    [ 100, 50, 0 ],\n",
    "    [ 10 ]\n",
    "  ],\n",
    "  \"radius_or_number_of_bins\": [ 32, 32 ],\n",
    "  \"sampling_percentage\": [ 0.05, 0.1 ],\n",
    "  \"sampling_strategy\": [ \"Regular\", \"Random\" ],\n",
    "  \"shrink_factors\": [\n",
    "    [ 2, 2, 1 ],\n",
    "    [ 1 ]\n",
    "  ],\n",
    "  \"sigma_units\": [ \"vox\", \"vox\" ],\n",
    "  \"smoothing_sigmas\": [\n",
    "    [ 4.0, 2.0, 0.0 ],\n",
    "    [ 0.0 ]\n",
    "  ],\n",
    "  \"transform_parameters\": [\n",
    "    [ 0.01 ],\n",
    "    [ 0.01 ]\n",
    "  ],\n",
    "  \"transforms\": [ \"Rigid\", \"Rigid\" ],\n",
    "  \"use_estimate_learning_rate_once\": [ false, true ],\n",
    "  \"use_histogram_matching\": [ true, true ],\n",
    "  \"verbose\": true,\n",
    "  \"winsorize_lower_quantile\": 0.0001,\n",
    "  \"winsorize_upper_quantile\": 0.9998,\n",
    "  \"write_composite_transform\": false\n",
    "}\n",
    "```\n",
    "\n",
    "Yes, configuring image registration is definitely not *straightforward*.\n",
    "The most relevant piece of settings to highlight is the `\"transforms\"` key, where we can observe we will be using a `\"Rigid\"` transform model.\n",
    "\n",
    "## Example registration\n",
    "It is beyond the scope of this tutorial to understand ANTs and/or image registration altogether, but let's have a look at how registration is integrated.\n",
    "First, we'll need to generate one target gradient prediction following all the steps learned previously.\n",
    "For this example, we have selected the 8<sup>th</sup> DW map (`index=7`) because it contains a sudden motion spike, resembling a nodding movement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "56a4e643",
   "metadata": {},
   "outputs": [],
   "source": [
    "from eddymotion.model import ModelFactory\n",
    "\n",
    "data_train, data_test = dmri_dataset.logo_split(7, with_b0=True)\n",
    "\n",
    "model = ModelFactory.init(\n",
    "    gtab=data_train[1],\n",
    "    model=\"DTI\",\n",
    "    S0=dmri_dataset.bzero,\n",
    ")\n",
    "model.fit(data_train[0])\n",
    "predicted = model.predict(data_test[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e05fcf8",
   "metadata": {},
   "source": [
    "Since we are using the command-line interface of ANTs, the software must be installed in the computer and the input data is provided via files in the filesystem.\n",
    "Let's write out two NIfTI files in a temporary folder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7d9975cd",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name '_to_nifti' from 'eddymotion.estimator' (/opt/hostedtoolcache/Python/3.7.10/x64/lib/python3.7/site-packages/eddymotion/estimator.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-f9f54fccc673>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpathlib\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPath\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtempfile\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmkdtemp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0meddymotion\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimator\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_to_nifti\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtempdir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmkdtemp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name '_to_nifti' from 'eddymotion.estimator' (/opt/hostedtoolcache/Python/3.7.10/x64/lib/python3.7/site-packages/eddymotion/estimator.py)"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from tempfile import mkdtemp\n",
    "from eddymotion.estimator import _to_nifti\n",
    "\n",
    "tempdir = Path(mkdtemp())\n",
    "\n",
    "# The fixed image is our prediction\n",
    "fixed_path = tempdir / \"fixed.nii.gz\"\n",
    "_to_nifti(predicted, dmri_dataset.affine, fixed_path)\n",
    "\n",
    "# The moving image is the left-out DW map\n",
    "moving_path = tempdir / \"moving.nii.gz\"\n",
    "_to_nifti(data_test[0], dmri_dataset.affine, moving_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3170a399",
   "metadata": {},
   "source": [
    "We can now visualize our reference (the prediction) and the actual DW map.\n",
    "Please notice the subtle *nodding* of the head, perhaps more apparent when looking at the corpus callosum in the sagittal views:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f5280df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from niworkflows.viz.notebook import display\n",
    "\n",
    "display(\n",
    "    fixed_path,\n",
    "    moving_path,\n",
    "    fixed_label=\"Predicted\",\n",
    "    moving_label=\"Left-out gradient\",\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6e3be16",
   "metadata": {},
   "source": [
    "Let's configure ANTs via NiPype:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fef0f46a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import cpu_count\n",
    "from pkg_resources import resource_filename as pkg_fn\n",
    "from nipype.interfaces.ants.registration import Registration\n",
    "\n",
    "registration = Registration(\n",
    "    terminal_output=\"file\",\n",
    "    from_file=pkg_fn(\n",
    "        \"eddymotion\",\n",
    "        f\"config/dwi-to-dwi_level1.json\",\n",
    "    ),\n",
    "    fixed_image=str(fixed_path.absolute()),\n",
    "    moving_image=str(moving_path.absolute()),\n",
    ")\n",
    "registration.inputs.output_warped_image = True\n",
    "registration.inputs.num_threads = cpu_count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c3634b2",
   "metadata": {},
   "source": [
    "which will run the following command-line:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4f6bc32",
   "metadata": {},
   "outputs": [],
   "source": [
    "registration.cmdline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5c5ec0d",
   "metadata": {},
   "source": [
    "Nipype interfaces can be submitted for execution with the `run()` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55c0ac22",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = registration.run(cwd=str(tempdir.absolute()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc7383b3",
   "metadata": {},
   "source": [
    "If everything worked out, we can now retrieve the aligned file with the output `result.outputs.warped_image`:\n",
    "We can now visualize how close (or far) the two images are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d31131e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(\n",
    "    fixed_path,\n",
    "    result.outputs.warped_image,\n",
    "    fixed_label=\"Predicted\",\n",
    "    moving_label=\"Aligned\",\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "428a9072",
   "metadata": {},
   "source": [
    "## Resampling an image\n",
    "\n",
    "Once we have estimated what is the *transform* that brings two images into alignment, we can *bring* the data in the *moving* image and *move this image* into the *reference*'s grid through *resampling*.\n",
    "\n",
    "The process works as follows:\n",
    "\n",
    "![nitransforms](https://raw.githubusercontent.com/poldracklab/nitransforms/master/docs/_static/figure1-joss.png)\n",
    "\n",
    "[*NiTransforms*](https://doi.org/10.1109/ISBI45749.2020.9098466) ([open-access pre-print here](https://doi.org/10.31219/osf.io/8aq7b)) is an ongoing project to bring a compatibility layer into *NiBabel* between the many transform file formats generated by neuroimaging packages.\n",
    "We will be using *NiTransforms* to *apply* these transforms we estimate with ANTs -- effectively *resampling* moving images into their reference's grid.\n",
    "\n",
    "To read a transform produced by ANTs with *NiTransforms*, we use the following piece of code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5a3e484",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nitransforms as nt\n",
    "\n",
    "itk_xform = nt.io.itk.ITKLinearTransform.from_filename(result.outputs.forward_transforms[0])\n",
    "matrix = itk_xform.to_ras(reference=fixed_path, moving=moving_path)\n",
    "matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6de689f2",
   "metadata": {},
   "source": [
    "Resampling an image requires two pieces of information: the *reference* image (which provides the new grid where we want to have the data) and the *moving* image which contains the actual data we are interested in:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3178c5e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "xfm = nt.linear.Affine(matrix)\n",
    "xfm.reference = fixed_path\n",
    "resampled = xfm.apply(moving_path)\n",
    "resampled.to_filename(tempdir / \"resampled-nitransforms.nii.gz\")\n",
    "display(\n",
    "    fixed_path,\n",
    "    tempdir / \"resampled-nitransforms.nii.gz\",\n",
    "    fixed_label=\"Predicted\",\n",
    "    moving_label=\"Aligned (nitransforms)\",\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feb8a265",
   "metadata": {},
   "source": [
    "**Exercise**\n",
    "Use the `display()` function to visualize the image aligned as generated by ANTs vs. that generated by *NiTransforms* -- they should be aligned!.\n",
    "\n",
    "**Solution**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55205ec5",
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "display(\n",
    "    result.outputs.warped_image,\n",
    "    tempdir / \"resampled-nitransforms.nii.gz\",\n",
    "    fixed_label=\"Aligned (ANTs)\",\n",
    "    moving_label=\"Aligned (nitransforms)\",\n",
    ");"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "md:myst",
   "text_representation": {
    "extension": ".md",
    "format_name": "myst"
   }
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "source_map": [
   11,
   15,
   37,
   124,
   136,
   140,
   154,
   158,
   167,
   171,
   187,
   190,
   192,
   195,
   197,
   201,
   208,
   223,
   229,
   233,
   244,
   250
  ]
 },
 "nbformat": 4,
 "nbformat_minor": 5
}